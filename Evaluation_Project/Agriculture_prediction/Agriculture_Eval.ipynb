{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Tue Feb 28"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@author: Kuldeep Kumar\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.impute import SimpleImputer\n", "from matplotlib import pyplot\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "import xgboost as xgb\n", "from sklearn.svm import SVC\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble import ExtraTreesClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_train = pd.read_excel(r'train_agriculture.xlsx')\n", "df_test = pd.read_excel(r'test_agriculture.xlsx')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Column headings:\")\n", "print(df_train.columns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Look into Data\",df_train.head(10))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Shape of the train dataset\",df_train.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Missing value in dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Missing Value in train data set\" ,df_train.isnull().sum())\n", "print(\"\\n Missing Value in test data set\" ,df_test.isnull().sum())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Class distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Class Distribution counts \",df_train['Crop_Damage'].value_counts())\n", "print(\"\\n Class Distribution counts \",df_train.groupby('Crop_Damage').size())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Concatenation of Data train and test after that split of data will do and Seprate the X and Y<br>\n", "rint(df_train.shape)<br>\n", "rint(df_test.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_train['train_test']='training_data'\n", "df_test['train_test']='testing_data'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_data = pd.concat((df_train, df_test),sort=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Look into data after concatenate\", df_data.head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Null value check in dataset after concatenate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Null Valve in dataset after concatenate\", df_data.isnull().sum())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n", "imp_mean.fit(df_data['Number_Weeks_Used'].values.reshape(-1,1))\n", "imp_mean.fit(df_data['Crop_Damage'].values.reshape(-1,1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_data['Number_Weeks_Used'] = imp_mean.transform(df_data['Number_Weeks_Used'].values.reshape(-1,1))\n", "df_data['Crop_Damage'] = imp_mean.transform(df_data['Crop_Damage'].values.reshape(-1,1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Null Valve in dataset after Impute\", df_data.isnull().sum())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look into data after impute nan with mean values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Look into data after impute nan with mean values \",df_data.head(10))\n", "print(\"Colums after impute\", df_data.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Drop Id field having no value added for data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_data.drop('ID',axis=1,inplace=True)\n", "print(\"Data and shape after drop of id\",df_data.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split-out validation dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_data=df_data.loc[df_data.train_test.isin(['training_data'])]\n", "test_data=df_data.loc[df_data.train_test.isin(['testing_data'])]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_data.drop(['train_test'],axis=1,inplace=True)\n", "test_data.drop(['train_test'],axis=1,inplace=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"train and test shape\",train_data.shape, test_data.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["visualize the correlations between the attributes.<br>\n", "correlation matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,12))\n", "sns.heatmap(train_data.corr(),linewidths=.1,cmap=\"Purples\", annot=True, annot_kws={\"size\": 7})\n", "plt.yticks(rotation=0)\n", "plt.savefig(\"corr.png\", format='png', dpi=900, bbox_inches='tight')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, Y_train = train_data.drop([\"Crop_Damage\"], axis=1).values, train_data[\"Crop_Damage\"].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=44)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Shape of test and train data\", X_train.shape, Y_train.shape, X_test.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = []\n", "models.append((\"LR\",LogisticRegression()))\n", "models.append((\"NB\",GaussianNB()))\n", "models.append((\"RF\",RandomForestClassifier()))\n", "models.append((\"SVC\",SVC()))\n", "models.append((\"Dtree\",DecisionTreeClassifier()))\n", "models.append((\"XGB\",xgb.XGBClassifier()))\n", "models.append((\"KNN\",KNeighborsClassifier()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = []\n", "names = []\n", "for name,model in models:\n", "    kfold = KFold(n_splits=2, random_state=22,shuffle=True)\n", "    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n", "    #print(name, cv_result)\n", "    results.append(cv_result)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_result.mean(), cv_result.std())\n", "    print(msg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Output for Accuracy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["R: 0.836912 (0.000044)<br>\n", "B: 0.817613 (0.002496)<br>\n", "F: 0.825225 (0.003486)<br>\n", "VC: 0.832563 (0.000046)<br>\n", "tree: 0.742595 (0.005638)<br>\n", "GB: 0.815439 (0.001409)<br>\n", "NN: 0.818972 (0.003311)<br>\n", "ccuracy: 0.837 (0.000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = pyplot.figure()\n", "fig.suptitle('Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["######### Logistic Regression Model #########"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Lkfold = KFold(n_splits=2, random_state=22,shuffle=True)\n", "Lmodel = LogisticRegression()\n", "#################Accuracy Check for Logistic as per spot check of Algo##########\n", "scoring = 'accuracy'\n", "results = cross_val_score(Lmodel, X_train,y_train, cv=Lkfold, scoring=scoring)\n", "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n", "#########prediction on test data#######\n", "ns_probs = [0 for _ in range(len(y_test))]\n", "Lmodel.fit(X_train, y_train)\n", "lr_probs = Lmodel.predict_proba(X_test)\n", "preds=np.argmax(lr_probs,axis=1)\n", "##########prediction on full data and contenation with ID########\n", "a=test_data.drop(['Crop_Damage'],axis=1)\n", "lr_probs_test = Lmodel.predict_proba(a)\n", "preds_test=np.argmax(lr_probs_test,axis=1)\n", "test_data['Crop_Damage']=preds_test\n", "combine_test= pd.concat([df_test['ID'], test_data['Crop_Damage']], axis=1)\n", "combine_test.to_csv(\"FinalSubmitOutput.csv\",index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["######################<br>\n", "Standardize the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_folds=2\n", "seed=22"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pipelines = []\n", "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n", "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n", "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('EN',RandomForestClassifier())])))\n", "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVC', SVC())])))\n", "pipelines.append(('ScaledDtree', Pipeline([('Scaler', StandardScaler()),('Dtree',DecisionTreeClassifier())])))\n", "pipelines.append(('ScaledXGB', Pipeline([('Scaler', StandardScaler()),('XGB',xgb.XGBClassifier())])))\n", "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = []\n", "names = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, model in pipelines:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "# Output is\n", "#ScaledLR: 0.828214 (0.000047)\n", "#ScaledNB: 0.817613 (0.002496)\n", "#ScaledRF: 0.823321 (0.001679)\n", "#ScaledSVC: 0.839359 (0.001859)\n", "#ScaledDtree: 0.743410 (0.004823)\n", "#ScaledXGB: 0.815439 (0.001409)\n", "#ScaledKNN: 0.828487 (0.003487)\n", "## Compare Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = pyplot.figure()\n", "fig.suptitle('Scaled Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fter scaled of data SVC is having more accuracy that 83% now do the Tunning of SVC<br>\n", "and its observed from output that it is lightly inproved 83 to 84 going with Ensemble<br>\n", "technique before finalizing the Model<br>\n", "Tune scaled SVM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n", "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n", "param_grid = dict(C=c_values, kernel=kernel_values)\n", "model = SVC()\n", "kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(rescaledX, y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["utput is<br>\n", "est: 0.840175 using {'C': 2.0, 'kernel': 'rbf'}<br>\n", ".832563 (0.000046) with: {'C': 0.1, 'kernel': 'linear'}<br>\n", ".839903 (0.002403) with: {'C': 0.1, 'kernel': 'poly'}<br>\n", ".832563 (0.000046) with: {'C': 0.1, 'kernel': 'rbf'}<br>\n", ".831476 (0.001041) with: {'C': 0.1, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 0.3, 'kernel': 'linear'}<br>\n", ".839903 (0.002403) with: {'C': 0.3, 'kernel': 'poly'}<br>\n", ".836368 (0.001675) with: {'C': 0.3, 'kernel': 'rbf'}<br>\n", ".797230 (0.007556) with: {'C': 0.3, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 0.5, 'kernel': 'linear'}<br>\n", ".839903 (0.002403) with: {'C': 0.5, 'kernel': 'poly'}<br>\n", ".838000 (0.000500) with: {'C': 0.5, 'kernel': 'rbf'}<br>\n", ".784726 (0.007552) with: {'C': 0.5, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 0.7, 'kernel': 'linear'}<br>\n", ".839903 (0.002403) with: {'C': 0.7, 'kernel': 'poly'}<br>\n", ".838815 (0.001315) with: {'C': 0.7, 'kernel': 'rbf'}<br>\n", ".776029 (0.012443) with: {'C': 0.7, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 0.9, 'kernel': 'linear'}<br>\n", ".839359 (0.002403) with: {'C': 0.9, 'kernel': 'poly'}<br>\n", ".839087 (0.001587) with: {'C': 0.9, 'kernel': 'rbf'}<br>\n", ".769778 (0.013800) with: {'C': 0.9, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 1.0, 'kernel': 'linear'}<br>\n", ".839359 (0.002403) with: {'C': 1.0, 'kernel': 'poly'}<br>\n", ".839359 (0.001859) with: {'C': 1.0, 'kernel': 'rbf'}<br>\n", ".768691 (0.013800) with: {'C': 1.0, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 1.3, 'kernel': 'linear'}<br>\n", ".839359 (0.002403) with: {'C': 1.3, 'kernel': 'poly'}<br>\n", ".838815 (0.001859) with: {'C': 1.3, 'kernel': 'rbf'}<br>\n", ".764342 (0.013798) with: {'C': 1.3, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 1.5, 'kernel': 'linear'}<br>\n", ".839087 (0.002131) with: {'C': 1.5, 'kernel': 'poly'}<br>\n", ".839359 (0.001859) with: {'C': 1.5, 'kernel': 'rbf'}<br>\n", ".757003 (0.013525) with: {'C': 1.5, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 1.7, 'kernel': 'linear'}<br>\n", ".839087 (0.002131) with: {'C': 1.7, 'kernel': 'poly'}<br>\n", ".839631 (0.002131) with: {'C': 1.7, 'kernel': 'rbf'}<br>\n", ".758090 (0.014612) with: {'C': 1.7, 'kernel': 'sigmoid'}<br>\n", ".832563 (0.000046) with: {'C': 2.0, 'kernel': 'linear'}<br>\n", ".838815 (0.001859) with: {'C': 2.0, 'kernel': 'poly'}<br>\n", ".840175 (0.002675) with: {'C': 2.0, 'kernel': 'rbf'}<br>\n", ".754557 (0.013796) with: {'C': 2.0, 'kernel': 'sigmoid'}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nsemble Methods : improve the performance of algorithms on this problem<br>\n", "our di\u000berent ensemble machine learning<br>\n", "lgorithms, two boosting and two bagging methods:<br>\n", "nsemble Methods<br>\n", "1. Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).<br>\n", "2. Bagging Methods: Random Forests (RF) and Extra Trees (ET)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["ensembles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ensembles = []\n", "ensembles.append(('AB', AdaBoostClassifier()))\n", "ensembles.append(('GBM', GradientBoostingClassifier()))\n", "ensembles.append(('RF', RandomForestClassifier()))\n", "ensembles.append(('ET', ExtraTreesClassifier()))\n", "results = []\n", "names = []\n", "for name, model in ensembles:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["utput is GBM is getting highest value that 83 % which is equal to our<br>\n", "previos model that was creted using Logistic Regression so we can try this also to finalize the model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["AB: 0.832020 (0.000046)<br>\n", "BM: 0.836369 (0.003217)<br>\n", "F: 0.822507 (0.001854)<br>\n", "T: 0.815168 (0.004299)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_trees = 100\n", "kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n", "gbmmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n", "results = cross_val_score(model,  X_train, y_train, cv=kfold)\n", "print(\"Mean and Standard Deviation for GBM\",results.mean(),results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#######prediction on test data#######"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ns_probs = [0 for _ in range(len(y_test))]\n", "gbmmodel.fit(X_train, y_train)\n", "lr_probs = gbmmodel.predict_proba(X_test)\n", "preds=np.argmax(lr_probs,axis=1)\n", "##########prediction on full data and contenation with ID########\n", "a=test_data.drop(['Crop_Damage'],axis=1)\n", "lr_probs_test = gbmmodel.predict_proba(a)\n", "preds_test=np.argmax(lr_probs_test,axis=1)\n", "test_data['Crop_Damage']=preds_test\n", "combine_test= pd.concat([df_test['ID'], test_data['Crop_Damage']], axis=1)\n", "combine_test.to_csv(\"FinalGBMSubmitOutput.csv\",index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}